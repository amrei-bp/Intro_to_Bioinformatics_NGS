[
  {
    "objectID": "NGS2.html",
    "href": "NGS2.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "NGS2.html#connect-to-dardel",
    "href": "NGS2.html#connect-to-dardel",
    "title": "",
    "section": "Connect to Dardel",
    "text": "Connect to Dardel\nFor this tutorial we will connect to Dardel. For everyone connecting via Kerberos this is the command:\nssh -o GSSAPIAuthentication=yes &lt;PDC username&gt;@dardel.pdc.kth.se"
  },
  {
    "objectID": "NGS2.html#start-a-screen-session",
    "href": "NGS2.html#start-a-screen-session",
    "title": "",
    "section": "Start a screen session",
    "text": "Start a screen session\nScreen or GNU Screen is a terminal multiplexer. In other words, it means that you can start a screen session and then open any number of windows (virtual terminals) inside that session. Processes running in Screen will continue to run when their window is not visible even if you get disconnected.\nStart a named session\nscreen -S mapping\nYou can detach from the screen session. The process within the screen will continue to run.\nCtrl + a d\nYou can always reattach to the session. If you have a number of screen running, or are unsure of the name or ID of the screen you want to reattach to you can list the currently running screens:\nscreen -ls\nTo resume your screen session use the following command:\nscreen -r name\n\n\n\n\n\n\nTipExercise\n\n\n\nCan you find the screen session that you started last friday?"
  },
  {
    "objectID": "NGS2.html#change-into-pdc-scratch-space",
    "href": "NGS2.html#change-into-pdc-scratch-space",
    "title": "",
    "section": "Change into PDC scratch space",
    "text": "Change into PDC scratch space\nTo move into the scratch space, change into it:\ncd $PDC_TMP\nYou can check that you are in it by printing your working directory:\npwd\n\n/cfs/klemming/scratch/&lt;user letter&gt;/&lt;user name&gt;\n\n\n\n\n\n\n\nTipExercise\n\n\n\nDo you remember why we decided to work from scratch?"
  },
  {
    "objectID": "NGS2.html#create-a-directory-to-work-in",
    "href": "NGS2.html#create-a-directory-to-work-in",
    "title": "",
    "section": "Create a directory to work in",
    "text": "Create a directory to work in\nStart by creating a workspace for this exercise in your scratch folder, and then move into it:\nmkdir -p  NGS_course/mapping/ref\ncd NGS_course/mapping/ref"
  },
  {
    "objectID": "NGS2.html#symbolic-links-to-data",
    "href": "NGS2.html#symbolic-links-to-data",
    "title": "",
    "section": "Symbolic links to data",
    "text": "Symbolic links to data\nTo save time and computation power we will use only chromosome01 for the exercise.\nThe reference data files are located in:\n/sw/courses/slu_bioinfo\nCreate symbolic links to the fastq files in your workspace:\nln -s /sw/courses/slu_bioinfo/chromosome01.fasta  .\nln -s /sw/courses/slu_bioinfo/Mesculenta_671_v8.1.gene_exons.gtf .\nln -s /sw/courses/slu_bioinfo/Mesculenta_671_v8.1.gene_exons.gff3 .\n\n\n\n\n\n\nTipExercise\n\n\n\nDo you remember why we decided to link to the files instead of copying them?\n\n\nCheck that you can now see the linked files in the directory and move back into the directory “mapping”.\nls\ncd .."
  },
  {
    "objectID": "NGS2.html#bowtie-2",
    "href": "NGS2.html#bowtie-2",
    "title": "",
    "section": "Bowtie 2",
    "text": "Bowtie 2\nNow we are all set to map our paired end DNA sequences onto the reference.\nBowtie2 is an ultrafast and memory-efficient tool for aligning sequencing reads to long reference sequences. It is particularly good at aligning reads of about 50 up to 100s or 1,000s of characters, and particularly good at aligning to relatively long (e.g. mammalian) genomes. Bowtie 2 indexes the genome with an FM Index to keep its memory footprint small: for the human genome, its memory footprint is typically around 3.2 GB. Bowtie 2 supports gapped, local, and paired-end alignment modes."
  },
  {
    "objectID": "NGS2.html#apptainer-image",
    "href": "NGS2.html#apptainer-image",
    "title": "",
    "section": "Apptainer image",
    "text": "Apptainer image\nGo to seqera containers, and get the container image path for bioconda::bowtie2. Remember to get the singularity container, not the one for docker.\nPull the image.\n\n\n\n\n\n\nCautionExpand to see the code - try yourself first\n\n\n\n\n\nsingularity pull --name bowtie2_2.5.4.sif https://community-cr-prod.seqera.io/docker/registry/v2/blobs/sha256/9b/9bbc1c148cefc585f681c71d9369a84f72f46ff1195850c7d416f9bbf65cb51e/data"
  },
  {
    "objectID": "NGS2.html#create-index",
    "href": "NGS2.html#create-index",
    "title": "",
    "section": "Create index",
    "text": "Create index\nEach NGS mapper uses indices to accelerate the mapping process, but unfortunately each one has his own way to create these indices. Therefore we first need to create the index for bowtie2 of our reference sequence, which we will call Bowtie2Cassava01Index and will be located in the reference directory.\nCreate a bash file within the scripts directory:\nnano bowtie_index.sh\nAnd copy the followiing into the file:\n#! /bin/bash -l\n\n#SBATCH -A edu24.bk0001\n#SBATCH -t 05:00\n#SBATCH -n 1\n#SBATCH -p shared\n\nset -ueo pipefail\n\nmodule load PDC apptainer\n\n# Define input reference and output directory\nREFERENCE=ref/chromosome01.fasta\nOUTPUT_DIR=ref/\n\nsingularity exec  -B /sw/courses/slu_bioinfo/ ../singularity_images/bowtie2_2.5.4.sif \\\n    bowtie2-build -o 3 $REFERENCE $OUTPUT_DIR/Bowtie2Cassava01Index\n    \necho \"complete\"\nSave the above code and run the script using\nsbatch ../scripts/bowtie_index.sh\n\n\n\n\n\n\nImportant\n\n\n\nMake sure you run this code from within the “mapping directory”.\n\n\nYou will get the following index files:"
  },
  {
    "objectID": "NGS2.html#map-with-bowtie2",
    "href": "NGS2.html#map-with-bowtie2",
    "title": "",
    "section": "Map with bowtie2",
    "text": "Map with bowtie2\nNow that we have the index we create a directory for the bowtie2 results:\nmkdir bowtie2\nand then we can run the mapping:\nMake a file in the scripts folder bowtie2_mapping.sh.\n#! /bin/bash -l\n\n#SBATCH -A edu24.bk0001\n#SBATCH -t 15:00\n#SBATCH -n 4\n#SBATCH -p shared\n\nset -ueo pipefail\n\nmodule load PDC apptainer\n\n# Get CPUS allocated to slurm script (-n above)\nCPUS=$SLURM_NPROCS\n\nINDEX=ref/Bowtie2Cassava01Index\nR1=../raw/TMEB117_R1_frac.fastq\nR2=../raw/TMEB117_R2_frac.fastq\n\nsingularity exec  -B /sw/courses/slu_bioinfo/ -B $PWD/../raw ../singularity_images/bowtie2_2.5.4.sif \\\n    bowtie2 --very-sensitive-local \\\n    --threads $CPUS \\\n    -x $INDEX \\\n    -S bowtie2/TMEB117.sam \\\n    -1 $R1 \\\n    -2 $R2 |& tee bowtie2/TMEB117.log\n\necho \"complete\"\nSave the script in the scripts directory and run the code.\nWe will get a mapping file in the SAM format and the log file with the mapping statistics.\n\nmapping/bowtie2/TMEB117.sam\nmapping/bowtie2/TMEB117.log\n\n\n\n\n\n\n\nTipExercise\n\n\n\nCheck the log and find how many unique hits of pairs we have. What does that even mean?\n\n\nCheck the SAM file and check how many hits we have - use samtools to do so. Module load the tool, or get the appropriate container. Look into the samtools manual to figure out how to look at the file.\nNow you can play around a bit. You could see what influence the trimming might have on the mapping of these data. For that re-run the mapping with the trimmed data. Make sure you point towards the correct file paths!\nCheck the log and SAM file and see what difference we have.\nTime allowing, I propose to run a third test mapping either using the raw data of trimmed data by changing the mapping mode from local to end-to-end or from very-sensitive to fast, loosing some accuracy but reducing the mapping time.\nReplace in the commands above –very-sensitive-local to –very-sensitive or from –very-sensitive-local to –fast-local.\nAgain, compare the output with the previous ones."
  },
  {
    "objectID": "NGS2.html#star",
    "href": "NGS2.html#star",
    "title": "",
    "section": "STAR",
    "text": "STAR\nSTAR is one such spicing site sensitive mapper for RNA sequencing data."
  },
  {
    "objectID": "NGS2.html#apptainer-image-1",
    "href": "NGS2.html#apptainer-image-1",
    "title": "",
    "section": "Apptainer image",
    "text": "Apptainer image\nGo to seqera containers, and get the container image path for bioconda::star.\nPull the image into the same directory as the other container images.\n\n\n\n\n\n\nNote\n\n\n\nWhich one was that again?\n\n\n\n\n\n\n\n\nCautionExpand to see the code for the pull - try yourself first\n\n\n\n\n\nsingularity pull --name star_2.7.11b.sif https://community-cr-prod.seqera.io/docker/registry/v2/blobs/sha256/9b/9b8ecb2f9a77b5e7573ef6fae2f4c2e771064f7a129ed1329913c1025c33f365/data"
  },
  {
    "objectID": "NGS2.html#prepare-index",
    "href": "NGS2.html#prepare-index",
    "title": "",
    "section": "Prepare index",
    "text": "Prepare index\nThe data we will use for this part of the tutorial is paired end RNA sedquencing data and is already in your /raw directory (TMEB419RNA_frag_R1.fastq, TMEB419RNA_frag_R2.fastq).\nFirst, we need to generate the STAR index, and this time we will include gene annotations to enable the mapped reads to be associated with known genes and transcripts.\nNext we have to create the directory where we will store the STAR index for chromosome 1. Let’s use the /refdirectory for that:\nmkdir ref/STARCassava01Index\nThe indexing script for STAR will be the following:\n\n\n\n\n\n\nTipExercise\n\n\n\nDo you remember all the steps you need to do to run the script?\n\n\n\n\n\n\n\n\nCautionExpand to see the code for the pull - try yourself first\n\n\n\n\n\nSave the below in a file:\nstar_index.sh\nin the appropriate directory, and run with\nsbatch star_index.sh\nfrom the appropriate directory.\n\n\n\n#! /bin/bash -l\n\n#SBATCH -A edu24.bk0001\n#SBATCH -t 05:00\n#SBATCH -n 1\n#SBATCH -p shared\n\nset -ueo pipefail\n\nmodule load PDC apptainer\n\nINDEX=ref/STARCassava01Index\nREFERENCE=ref/chromosome01.fasta\nGTF=ref/Mesculenta_671_v8.1.gene_exons.gtf\n\nsingularity exec  -B /sw/courses/slu_bioinfo/ -B $PWD/../raw ../singularity_images/star_2.7.11b.sif \\\n    STAR    --runMode genomeGenerate \\\n        --genomeDir $INDEX \\\n        --genomeFastaFiles $REFERENCE \\\n        --sjdbGTFfile $GTF \\\n        --genomeSAindexNbases 11 \\\n        --genomeChrBinNbits 16 \\\n        --outTmpDir ./star\nWith this you will end up with a bunch of index files:\n\nFiles like geneInfo.tab, exonGeTrInfo.tab, transcriptInfo.tab, exonInfo.tab, sjdbList.fromGTF.out.tab, sjdbList.out.tab, sjdbInfo.txt contain the indexing information of the predicted genes, transcripts, exons and splicing sites annotated in the Mesculenta_671_v8.1.gene_exons.gtf file. Have a look around!\n\n\n\n\n\n\nTipExercise\n\n\n\nHave a look around the files to see what you discover!"
  },
  {
    "objectID": "NGS2.html#map-with-star",
    "href": "NGS2.html#map-with-star",
    "title": "",
    "section": "Map with STAR",
    "text": "Map with STAR\nNow we are ready to map the DNA sequencing files to the reference sequence.\nMake a directory for the output:\nmkdir star\nAnd then save the following script and run it with sbatch (like above).\n#! /bin/bash -l\n\n#SBATCH -A edu24.bk0001\n#SBATCH -t 15:00\n#SBATCH -n 4\n#SBATCH -p shared\n\nset -ueo pipefail\n\nmodule load PDC apptainer\n\n# Get CPUS allocated to slurm script (-n above)\nCPUS=$SLURM_NPROCS\n\nINDEX=ref/STARCassava01Index\nREAD1=../raw/TMEB419RNA_frag_R1.fastq\nREAD2=../raw/TMEB419RNA_frag_R2.fastq\n\nsingularity exec  -B /sw/courses/slu_bioinfo/ -B $PWD/../raw ../singularity_images/star_2.7.11b.sif \\\n    STAR --genomeDir $INDEX \\\n        --outFilterMismatchNoverLmax 0.06 \\\n        --outFilterMatchNminOverLread 0.5 \\\n        --outFilterScoreMinOverLread 0.2 \\\n        --alignIntronMax 20000 \\\n        --alignMatesGapMax 10000 \\\n        --outFileNamePrefix star/TMEB419RNA_  \\\n        --readFilesIn $READ1 $READ2\n\necho \"complete\"\nRun with sbatch.\nYour output will be the SAM file, report files and the list of predicted splicing sites.\n\n\n\n\n\n\nTipExercise\n\n\n\nInspect the report file and check the mapped reads and the average mapped length.\nIs this a good mapping?"
  },
  {
    "objectID": "NGS2.html#footnotes",
    "href": "NGS2.html#footnotes",
    "title": "",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://www.ebi.ac.uk/training/online/courses/functional-genomics-ii-common-technologies-and-data-analysis-methods/rna-sequencing/performing-a-rna-seq-experiment/data-analysis/read-mapping-or-alignment/↩︎\nhttps://training.galaxyproject.org/training-material/topics/sequence-analysis/tutorials/mapping/tutorial.html↩︎"
  },
  {
    "objectID": "server_access.html",
    "href": "server_access.html",
    "title": "Create user accounts for the course server",
    "section": "",
    "text": "The following procedure takes a few days to get through because of clearance delays, so please start straight away, since every thing else in the course depends on it :)\nWe will run parts of this course on a High Performance Computing cluster, HPC2N. The cluster is a local resource in Umeå, and to use it you need a user account both at the National Academic Infrastructure for Super­computing in Sweden (NAISS), and at HPC2N."
  },
  {
    "objectID": "server_access.html#create-a-user-account-at-suprnaiss",
    "href": "server_access.html#create-a-user-account-at-suprnaiss",
    "title": "Create user accounts for the course server",
    "section": "Create a user account at SUPR/NAISS",
    "text": "Create a user account at SUPR/NAISS\n\n\n\n\n\n\nImportantTo do for you\n\n\n\nFill out & submit the account request form. Read carefully and follow the provided instructions.\n\n\n\n\n\n\n\n\nTip\n\n\n\n\nUse “Register with Federated Identity” if you are enrolled in a Swedish university.\nIf you are not enrolled in a Swedish university or if the previous option doesn’t work for you, use instead “Register without Federated Identity”.\n\n\n\n\nAccept NAISS user agreement\nJust after creating the SUPR account, on the page you are taken to, there will be a section titled “User Agreement” at the very top. Just under it is the button named “Accept NAISS User Agreement”.\n\n\n\n\n\n\nImportantTo do for you\n\n\n\nClick it.\n\n\nAlternately, if you do this at a later point. Login to SUPR. At the very top it will say “NAISS User Agreement Pending”.\n\n\n\n\n\n\nTip\n\n\n\n\nUse With SWAMID if you are enrolled in a Swedish university - this option is much faster!\nIf you are not enrolled in a Swedish university use instead Alternative: Offline via Paper Form. Here you will have to print out, sign and send in the form. This may take up to a week. \n\n\n\nWhen you have a SUPR account you can join our course project on HPC2N:"
  },
  {
    "objectID": "server_access.html#join-the-course-project-at-hpc2n",
    "href": "server_access.html#join-the-course-project-at-hpc2n",
    "title": "Create user accounts for the course server",
    "section": "Join the course project at HPC2N",
    "text": "Join the course project at HPC2N\n\n\n\n\n\n\nImportantTo do for you\n\n\n\nLog in to SUPR.\n\nClick “Projects” in the left side column.\nUnder “Requesting Membership in Projects”, put in hpc2n2025-237, the identification of the project you wish to join. Click “Search for Project”.\nClick “Request” on the project.\nWe will then get an e-mail notification about your request. Once we have accepted your membership, you in turn will get an e-mail, telling you that the membership has been accepted. Then, go to SUPR again to apply for an account at HPC2N."
  },
  {
    "objectID": "server_access.html#create-a-user-account-at-hpc2n",
    "href": "server_access.html#create-a-user-account-at-hpc2n",
    "title": "Create user accounts for the course server",
    "section": "Create a user account at HPC2N",
    "text": "Create a user account at HPC2N\nNote: you must be a member of a project before you do this!\n\n\n\n\n\n\nImportantTo do for you\n\n\n\n\nLogin to SUPR.\nClick “Accounts” in the left side column.\nYou can request an account at HPC2N now. Look under the heading “Account Requests”.\nClick “Request account”.\nYour information will then be sent to HPC2N, and you will be taken back to a webpage where you can choose your username.\nUser accounts are usually created once a week. You will get an email from HPC2N when your account has been created."
  },
  {
    "objectID": "server_access.html#questions",
    "href": "server_access.html#questions",
    "title": "Create user accounts for the course server",
    "section": "Questions?",
    "text": "Questions?\nContact me via canvas!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "",
    "section": "",
    "text": "Code\n\n\n\n\n\n\nThe data\nThe data we will be working on today are DNA sequencing data and RNA sequencing data from cassava cultivars.\nThe paired end DNA sequences were retreived from the cassava cultivar called TMEB117, a strain that whose genome is now known and that, due to its susceptibility to certain cassava diseases, can be used as a reference for studying virus resistance mechanisms in cassava.\nThe paired end RNA sequencing data comes from a cassava genotype called TMEB419. This cultivar was bred to be highly resistant to various cassava viruses and is used commercially.\nSpecial thanks to Andreas Gisel for providing these samples (and the picture above)!\n\n\nData location\nThe data is located on the server in the directory /sw/courses/slu_bioinfo."
  },
  {
    "objectID": "NGS1.html",
    "href": "NGS1.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "NGS1.html#connect-to-dardel",
    "href": "NGS1.html#connect-to-dardel",
    "title": "",
    "section": "Connect to Dardel",
    "text": "Connect to Dardel\nFor this tutorial we will connect to Dardel. For everyone connecting via Kerberos this is the command:\nssh -o GSSAPIAuthentication=yes &lt;PDC username&gt;@dardel.pdc.kth.se\nFor logging in via SSH keys, the command is the following:\nssh  &lt;PDC_username&gt;@dardel.pdc.kth.se"
  },
  {
    "objectID": "NGS1.html#start-a-screen-session",
    "href": "NGS1.html#start-a-screen-session",
    "title": "",
    "section": "Start a screen session",
    "text": "Start a screen session\nScreen or GNU Screen is a terminal multiplexer. In other words, it means that you can start a screen session and then open any number of windows (virtual terminals) inside that session. Processes running in Screen will continue to run when their window is not visible even if you get disconnected.\nStart a named session\nscreen -S qc\nYou can detach from the screen session. The process within the screen will continue to run.\nCtrl + a d\nYou can always reattach to the session. If you have a number of screen running, or are unsure of the name or ID of the screen you want to reattach to you can list the currently running screens:\nscreen -ls\nTo resume your screen session use the following command:\nscreen -r name"
  },
  {
    "objectID": "NGS1.html#change-into-pdc-scratch-space",
    "href": "NGS1.html#change-into-pdc-scratch-space",
    "title": "",
    "section": "Change into PDC scratch space",
    "text": "Change into PDC scratch space\nOn PDC course allocations do not get an assigned storage allocation. They expect us to work from our home directories. The home directory is where you land when you connect to Rackham. If you check your current working directory it will look something like this:\npwd\n\n/cfs/klemming/home/&lt;user letter&gt;/&lt;user name&gt;\n\nYou can always come back to your home directory by entering:\ncd\nThe home directories have a quota of 25 GB, so there is not much space in them.\nHowever, connected to our home directories, PDC has a temporary disk space, called scratch. The scratch area is intended for temporary large files that are used during calculations. There is no quota on the space, and it gets cleaned up after 30 days. This is where we will run our computations.\nTo move into the scratch space, change into it:\ncd $PDC_TMP\nYou can check that you are in it by printing your working directory:\npwd\n\n/cfs/klemming/scratch/&lt;user letter&gt;/&lt;user name&gt;"
  },
  {
    "objectID": "NGS1.html#create-a-directory-to-work-in",
    "href": "NGS1.html#create-a-directory-to-work-in",
    "title": "",
    "section": "Create a directory to work in",
    "text": "Create a directory to work in\nStart by creating a workspace for the raw data used in this exercise in your scratch space, and then move into it:\nmkdir -p  NGS_course/raw\ncd NGS_course/raw"
  },
  {
    "objectID": "NGS1.html#create-symbolic-link-to-the-data",
    "href": "NGS1.html#create-symbolic-link-to-the-data",
    "title": "",
    "section": "Create symbolic link to the data",
    "text": "Create symbolic link to the data\nThe raw data files are located in:\n/sw/courses/slu_bioinfo\nYou could copy the files into your workspace to access them. However, it is better to create symbolic links (also called soft links) to the data files. This saves disk space and still allows you to work with them as if they were in your own directory.\nCreate symbolic links to the fastq files in your workspace:\nln -s /sw/courses/slu_bioinfo/*.fastq .\nYou now have four files in your directory: two for the TMEB117 cultivar containing the DNA sequences, and two for the TMEB419 cultivar, containing the RNA sequencing results."
  },
  {
    "objectID": "NGS1.html#apptainer",
    "href": "NGS1.html#apptainer",
    "title": "",
    "section": "Apptainer",
    "text": "Apptainer\nThere are several ways to manage bioinformatics tools, such as using Conda, container platforms, or the module system, which you might have encountered in a previous course.\nIn this tutorial, we will focus on Apptainer — the open-source version of Singularity. By using Apptainer, we are flexible in running tools and specific tool versions that may not be directly installed on the system we are working on. All we need for this is a system where Apptainer is installed. Luckily for us, Dardel is one such system.\nLoad the module with\nmodule load PDC apptainer\nMake a directory for the output of the tool:\ncd .. \nmkdir fastqc\nCheck the directory you are in:\npwd \nYou should be located in\n\n/NGS_course"
  },
  {
    "objectID": "NGS1.html#getting-apptainer-image",
    "href": "NGS1.html#getting-apptainer-image",
    "title": "",
    "section": "Getting apptainer image",
    "text": "Getting apptainer image\nOne good place to get quality controlled Apptainer/Singularity containers that contain the tools we want to use is seqera containers.\ngo to their homepage\nin the searchbar, type in the tool you want - fastqc\n\n\nAdd the tool you want to have in your container (in this case fastqc from Bioconda).\nIn the container settings underneath the search bar, select Singularity and linux/amd64\nClick “get container”\n\n\nOnce the container is ready select HTTPS and copy the name of the image."
  },
  {
    "objectID": "NGS1.html#download-container-images",
    "href": "NGS1.html#download-container-images",
    "title": "",
    "section": "Download container images",
    "text": "Download container images\nTo have a nice and clean project directory we will make a new sub-directory that will contain all the singularity images we will use during this tutorial.\nmkdir singularity_images\ncd singularity_images\nNow we can pull the container image from its location into our folder:\nsingularity pull --name fastqc_0.12.1.sif https://community-cr-prod.seqera.io/docker/registry/v2/blobs/sha256/e0/e0c976cb2eca5fee72618a581537a4f8ea42fcae24c9b201e2e0f764fd28648a/data\nThen we move out of the directory again:\ncd .."
  },
  {
    "objectID": "NGS1.html#running-fastqc-with-sbatch",
    "href": "NGS1.html#running-fastqc-with-sbatch",
    "title": "",
    "section": "Running fastqc with sbatch",
    "text": "Running fastqc with sbatch\nDardel is using slurm as its jobmanager (as you have heard earlier today). We will now use slurm’s command sbatch to run fastqc with the container image.\nAgain, we want to maintain a clean and orderly project directory:\nIn your NGS_course folder, create a new directory called scripts, within this directory create a file called fastqc.sh.\nmkdir scripts\ncd scripts\nmodule load nano/7.2\nnano fastqc.sh\n\n\n\n\n\n\nTipNano\n\n\n\nNano is a Linux command line text editor. Commands are prefixed with ^or M characters. The caret symbol ^ represents the Ctrl key. For example, the ^X commands mean to press the Ctrl and X keys at the same time. The letter M represents the Alt key.\nMore information here.\n\n\nCopy the following into the file, and save the contents. Read through the file and try to understand what the different lines are doing.\n#! /bin/bash -l\n\n#SBATCH -A edu24.bk0001\n#SBATCH -t 15:00\n#SBATCH -n 4\n#SBATCH -p shared\n\nmodule load PDC apptainer\n\n# Get CPUS allocated to slurm script (-n above)\nCPUS=$SLURM_NPROCS\n\nsingularity exec -B /sw/courses/slu_bioinfo/ singularity_images/fastqc_0.12.1.sif \\\n    fastqc -t $CPUS -o fastqc/ raw/*.fastq\n    echo \"complete\"\nThe slurm options used here:\n\nA: project ID\nt allocated time dd-hh:mm:ss\nn number of cpus\np partition to use - here we will use the shared partition\n\nMove back into the NGS_course directory and submit the script to slurm:\ncd ..\nsbatch scripts/fastqc.sh\nAfter running a bash script you will get a slurm output. Look at that output. See if you understand what that output contains.\nless slurm-XXXXX.out\nLocate the output of FastQC.\n\n\n\n\n\n\nNote\n\n\n\nWhich output directory did you specify in the batch file?\n\n\nFor each fastq file you will get two output files:\n\nTMEB117_R1_frac_fastq.zip (report, data files and graphs)\n\n\nTMEB117_R1_frac_fastq.html (report in html)\n\nLet’s download both files to the local computer for consulting. Use a different terminal and navigate to where you want the files on your computer. Then copy the files with the following command (for Kerberos users):\nrsync -e \"ssh -o  GSSAPIAuthentication=yes\"  -ah &lt;user&gt;@dardel.pdc.kth.se:/cfs/klemming/scratch/&lt;user_letter&gt;/&lt;user&gt;/NGS_course/fastqc .\nSSH key users need to remove the -e \"ssh -o  GSSAPIAuthentication=yes\"part.\nLet’s look at the files. Go through the reports to understand your sample.\nYou see that it is getting kind of tedious to look through all the different files one by one. Okay with only a few files, but imagine having to sift through a few dozen, or even hundreds of reports."
  }
]