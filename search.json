[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "",
    "section": "",
    "text": "Code\n\n\n\n\n\n\nThe data\nThe data we will be working on today are DNA sequencing data and RNA sequencing data from cassava cultivars.\nThe paired end DNA sequences were retreived from the cassava cultivar called TMEB117, a strain that whose genome is now known and that, due to its susceptibility to certain cassava diseases, can be used as a reference for studying virus resistance mechanisms in cassava.\nThe paired end RNA sequencing data comes from a cassava genotype called TMEB419. This cultivar was bred to be highly resistant to various cassava viruses and is used commercially.\nSpecial thanks to Andreas Gisel for providing these samples (and the picture above)!\n\n\nData location\nThe data is located on the server in the directory /sw/courses."
  },
  {
    "objectID": "NGS2.html",
    "href": "NGS2.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "NGS1.html",
    "href": "NGS1.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "NGS1.html#connect-to-dardel",
    "href": "NGS1.html#connect-to-dardel",
    "title": "",
    "section": "Connect to Dardel",
    "text": "Connect to Dardel\nFor this tutorial we will connect to Dardel. For everyone connecting via Kerberos this is the command:\nssh -o GSSAPIAuthentication=yes &lt;PDC username&gt;@dardel.pdc.kth.se"
  },
  {
    "objectID": "NGS1.html#start-a-screen-session",
    "href": "NGS1.html#start-a-screen-session",
    "title": "",
    "section": "Start a screen session",
    "text": "Start a screen session\nScreen or GNU Screen is a terminal multiplexer. In other words, it means that you can start a screen session and then open any number of windows (virtual terminals) inside that session. Processes running in Screen will continue to run when their window is not visible even if you get disconnected.\nStart a named session\nscreen -S fastqc\nYou can detach from the screen session. The process within the screen will continue to run.\nCtrl + a d\nYou can always reattach to the session. If you have a number of screen running, or are unsure of the name or ID of the screen you want to reattach to you can list the currently running screens:\nscreen -ls\nTo resume your screen session use the following command:\nscreen name"
  },
  {
    "objectID": "NGS1.html#change-into-pdc-scratch-space",
    "href": "NGS1.html#change-into-pdc-scratch-space",
    "title": "",
    "section": "Change into PDC scratch space",
    "text": "Change into PDC scratch space\nOn PDC course allocations do not get an assigned storage allocation. They expect us to work from our home directories. The home directory is where you land when you connect to Rackham. If you check your current working directory it will look something like this:\npwd\n\n/cfs/klemming/home//\n\nYou can always come back to your home directory by entering:\ncd\nThe home directories have a quota of 25 GB, so there is not much space in them.\nHowever, connected to our home directories, PDC has a temporary disk space, called scratch. The scratch area is intended for temporary large files that are used during calculations. There is no quota on the space, and it gets cleaned up after 30 days. This is where we will run our computations.\nTo move into the scratch space, change into it:\ncd $PDC_TMP\nYou can check that you are in it by printing your working directory:\npwd\n\n/cfs/klemming/scratch//"
  },
  {
    "objectID": "NGS1.html#create-a-directory-to-work-in",
    "href": "NGS1.html#create-a-directory-to-work-in",
    "title": "",
    "section": "Create a directory to work in",
    "text": "Create a directory to work in\nStart by creating a workspace for the raw data used in this exercise in your scratch space, and then move into it:\nmkdir -p  NGI_course/qc/raw\ncd NGI_course/qc/raw"
  },
  {
    "objectID": "NGS1.html#create-symbolic-link-to-the-data",
    "href": "NGS1.html#create-symbolic-link-to-the-data",
    "title": "",
    "section": "Create symbolic link to the data",
    "text": "Create symbolic link to the data\nThe raw data files are located in:\n/sw/courses/slu_bioinfo\nYou could copy the files into your workspace to access them. However, it is better to create symbolic links (also called soft links) to the data files. This saves disk space and still allows you to work with them as if they were in your own directory.\nCreate symbolic links to the fastq files in your workspace:\nln -s /sw/courses/slu_bioinfo/*.fastq .\nYou now have four files in your directory: two for the TMEB117 cultivar containing the DNA sequences, and two for the TMEB419 cultivar, containing the RNA sequencing results."
  },
  {
    "objectID": "NGS1.html#running-fastqc",
    "href": "NGS1.html#running-fastqc",
    "title": "",
    "section": "Running fastqc",
    "text": "Running fastqc\nThere are several ways to manage bioinformatics tools, such as using Conda, container platforms, or the module system, which you might have encountered in a previous course.\nIn this tutorial, we will focus on Apptainer — the open-source version of Singularity. By using Apptainer, we are flexible in running tools and specific tool versions that may not be directly installed on the system we are working on. All we need for this is a system where Apptainer is installed. Luckily for us, Dardel is one such system.\nMake a directory for the output of the tool:\ncd .. \n\nmkdir fastqc\nCheck the directory you are in:\npwd \nYou should be located in\n\n/NGI_course/qc"
  },
  {
    "objectID": "NGS1.html#getting-apptainer-image",
    "href": "NGS1.html#getting-apptainer-image",
    "title": "",
    "section": "Getting apptainer image",
    "text": "Getting apptainer image\nOne good place to get quality controlled Apptainer/Singularity containers that contain the tools we want to use is seqera containers.\ngo to their homepage\nin the searchbar, type in the tool you want - fastqc\n\n\nAdd the tool you want to have in your container (in this case fastqc from Bioconda).\nIn the container settings underneath the search bar, select Singularity and linux/amd64\nClick “get container”\n\n\nOnce the container is ready select HTTPS and copy the name of the image."
  },
  {
    "objectID": "NGS1.html#download-container-images",
    "href": "NGS1.html#download-container-images",
    "title": "",
    "section": "Download container images",
    "text": "Download container images\nTo have a nice and clean project directory we will make a new sub-directory that will contain all the singularity images we will use during this tutorial.\nmkdir singularity_images\ncd singularity_images\nNow we can pull the container image from its location into our folder:\nsingularity pull --name fastqc_0.12.1.sif https://community-cr-prod.seqera.io/docker/registry/v2/blobs/sha256/e0/e0c976cb2eca5fee72618a581537a4f8ea42fcae24c9b201e2e0f764fd28648a/data\nThen we move out of the directory again:\ncd .."
  },
  {
    "objectID": "NGS1.html#running-fastqc-with-sbatch",
    "href": "NGS1.html#running-fastqc-with-sbatch",
    "title": "",
    "section": "Running fastqc with sbatch",
    "text": "Running fastqc with sbatch\nDardel is using slurm as its jobmanager (as you have heard earlier today). We will now use slurm’s command sbatch to run fastqc with the container image.\nAgain, we want to maintain a clean and orderly project directory:\nIn your rnaseq folder, create a new directory called scripts, within this directory create a file called fastqc.sh.\nmkdir scripts\ncd scripts\nnano fastqc.sh\nCopy the following into the file, and save the contents. Read through the file and try to understand what the different lines are doing.\n#! /bin/bash -l\n\n#SBATCH -A edu24.bk0001\n#SBATCH -t 15:00\n#SBATCH -n 4\n#SBATCH -p shared\n\nmodule load PDC apptainer\n\n# Get CPUS allocated to slurm script (-n above)\nCPUS=$SLURM_NPROCS\n\nsingularity exec -B /sw/courses/slu_bioinfo/ singularity_images/fastqc_0.12.1.sif \\\n    fastqc -t $CPUS -o fastqc/ raw/*.fastq\n    echo \"complete\"\nThe slurm options used here:\n\nA: project ID\nt allocated time dd-hh:mm:ss\nn number of cpus\np partition to use - here we will use the shared partition\n\nSubmit the script to slurm:\nsbatch fastqc.sh\nAfter running a bash script you will get a slurm output. Look at that output. See if you understand what that output contains.\nless slurm-XXXXX.out\nLocate the output of FastQC.\n\n\n\n\n\n\nNote\n\n\n\nWhich output directory did you specify in the batch file?\n\n\nFor each fastq file you will get two output files:\n\nTMEB117_R1_frac_fastq.zip (report, data files and graphs)\n\n\nTMEB117_R1_frac_fastq.html (report in html)\n\nLet’s download both files to the local computer for consulting. Use a different terminal and navigate to where you want the files on your computer. Then copy the files with the following command:\nrsync -ah &lt;user&gt;@dardel.pdc.kth.se:/cfs/klemming/scratch/&lt;user_letter&gt;/&lt;user&gt;/NGI_course/qc/fastqc .\nLet’s look at the files. Go through the reports to understand your sample.\nYou see that it is getting kind of tedious to look through all the different files one by one. Okay with only a few files, but imagine having to sift through a few dozen, or even hundreds of reports."
  },
  {
    "objectID": "NGS1.html#multiqc",
    "href": "NGS1.html#multiqc",
    "title": "",
    "section": "MultiQC",
    "text": "MultiQC\nMultiQC searches a given directory for analysis logs and compiles a HTML report. It’s a general use tool, perfect for summarising the output from numerous bioinformatics tools. It aggregates results from bioinformatics analyses across many samples into a single report.\n\nBuilding apptainer image\nOn the seqera container page choose bioconda::multiqc for your container image. Proceed to build the container image, following the steps we did for fastqc.\n\n\nDownload container images\nDownload the container image with singularity pull. The --name flag lets you re-name the image to a more intuitive name.\n\n\n\n\n\n\nNote\n\n\n\nGood practice is to name it after the tool and its version number.\n\n\nCopy the image into your singularity_images folder, if it isn’t there yet.\n\n\nRunning multiqc with sbatch\nWithin your scripts directory, make a new file, multiqc.sh, and add the following:\n#! /bin/bash -l\n\n#SBATCH -A edu24.bk0001\n#SBATCH -t 15:00\n#SBATCH -n 1\n#SBATCH -p shared\n\nmodule load PDC apptainer\n\nsingularity exec singularity_images/multiqc_1.25.1.sif \\\n    multiqc -f -o multiqc .\nNavigate out of the scripts directory back into the qc directory.\nMake a directory called multiqc.\nRun the bash script with\nsbatch scripts/multiqc.sh \nThe command output looks something like:\n/// MultiQC 🔍 v1.25.1\nconfig | Loading config settings from: multiqc_config.yaml file_search | Search path: /cfs/klemming/scratch/a/amrbin/NGI_course/qc\nfastqc | Found 4 reports\nwrite_results | Data : multiqc/multiqc_data (overwritten) write_results | Report : multiqc/multiqc_report.html (overwritten) multiqc | MultiQC complete\nDownload the report and look at it. Understand what is going on. Read the documentation.\nDo we need to adapter trim any samples?"
  },
  {
    "objectID": "NGS1.html#fastp",
    "href": "NGS1.html#fastp",
    "title": "",
    "section": "FastP",
    "text": "FastP\nFastP is a FASTQ data pre-processing tool. The algorithm has functions for quality control, trimming of adapters, filtering by quality, and read pruning.\nDependent on what analysis you need to do with the NGS data it is wise to process the data according to the quality control and remove low score sequences and/or low score 5’ and 3’ fragments. It makes sense to trim adapters for downstream analyses, but quality filtering can remove information that modern downstream tools can still utilize.\nLet’s get the output into a different directory:\nmkdir fastp\nThen retreive the container image from seqera containers:\nsingularity pull --name fastp_0.23.4.sif https://community-cr-prod.seqera.io/docker/registry/v2/blobs/sha256/3f/3fcff4f02e7e012e4bab124d64a2a50817dd64303998170127c8cf9c1968e10a/data\nMake sure the image is in the same folder as the other images we used so far.\nRun fastp with the following bash script:\n#! /bin/bash -l\n\n#SBATCH -A edu24.bk0001\n#SBATCH -t 15:00\n#SBATCH -n 4\n#SBATCH -p shared\n\nmodule load PDC apptainer\n\n# Get CPUS allocated to slurm script (-n above)\nCPUS=$SLURM_NPROCS\n\nsingularity exec -B /sw/courses/slu_bioinfo/ singularity_images/fastp_0.23.4.sif \\\n    fastp -i raw/TMEB117_R1_frac.fastq \\\n    -I raw/TMEB117_R2_frac.fastq \\\n    -o fastp/TMEB117_R1_frac_trim.fastq \\\n    -O fastp/TMEB117_R2_frac_trim.fastq \\\n    --json fastp/TMEB117_fastp.json \\\n    --html fastp/TMEB117_fastp.html\n\n    echo \"complete\"\n\n\n\n\n\n\nExercise\n\n\n\nDo you understand the bash script? Discuss with your neighbour and check out the manual for fastp.\n\n\n::: {.callout-tip} ## Exercise\nOnce you get the cleaned sequences run multiqc again to check the result. :::"
  }
]